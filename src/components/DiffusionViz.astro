---
/**
 * DiffusionViz.astro
 *
 * Visualizes the diffusion model denoising process used in image generation
 * (DALL-E, Stable Diffusion, Midjourney).
 *
 * Uses HTML5 Canvas at 64x64 resolution to render a proper 5-pointed star
 * that emerges from random noise over 7 denoising steps.
 * Center-outward denoising: pixels closer to center resolve first.
 *
 * GSAP ScrollTrigger with onUpdate maps scroll progress to step index.
 */

const STEPS = 7;

const stepLabels = [
  'Step 0: Pure noise',
  'Step 1: Faint structure',
  'Step 2: Shapes emerge',
  'Step 3: Colors coalesce',
  'Step 4: Pattern forming',
  'Step 5: Nearly resolved',
  'Step 6: Final image',
];
---

<section id="diffusion-section" class="diff-section" aria-label="Diffusion model denoising visualization">
  <div class="diff-container">
    <div class="diff-text-content">
      <h2 class="diff-heading" data-diff="heading">How image generation works</h2>
      <p class="diff-description" data-diff="desc">
        Diffusion models like DALL-E and Stable Diffusion learn to create images by
        reversing a noise process. Imagine ink dropped in water, diffusing until uniformly
        mixed. These models learn to <em>un-mix</em> the ink, step by step, guided by a
        text prompt.
      </p>
    </div>

    <div class="diff-viz-wrapper" data-diff="viz-wrapper">
      <!-- Text prompt display -->
      <div class="diff-prompt" data-diff="prompt">
        <span class="diff-prompt-icon" aria-hidden="true">&gt;</span>
        <span class="diff-prompt-text">"A snowman face on a blue background"</span>
      </div>

      <!-- Step indicator -->
      <div class="diff-step-indicator" data-diff="step-indicator">
        {stepLabels.map((label, i) => (
          <button
            class={`diff-step-dot ${i === 0 ? 'active' : ''}`}
            data-diff-step-btn={i}
            aria-label={label}
            title={label}
          >
            <span class="diff-step-dot-inner" />
          </button>
        ))}
      </div>

      <!-- Step label -->
      <div class="diff-step-label" data-diff="step-label" aria-live="polite">
        {stepLabels[0]}
      </div>

      <!-- Canvas visualization -->
      <div class="diff-grid-container" data-diff="grid-container">
        <canvas
          id="diffusion-canvas"
          class="diff-canvas"
          role="img"
          aria-label="Diffusion denoising visualization showing noise transforming into a snowman face on blue background"
          aria-describedby="diff-explanations"
        ></canvas>
      </div>

      <!-- Denoising arrow / progress bar -->
      <div class="diff-progress-track" data-diff="progress-track">
        <div class="diff-progress-fill" data-diff="progress-fill"></div>
        <div class="diff-progress-labels">
          <span class="diff-progress-label-start">Noise</span>
          <span class="diff-progress-label-end">Image</span>
        </div>
      </div>

      <!-- Explanation text for each step -->
      <div class="diff-step-explanations" id="diff-explanations" aria-live="polite">
        <p class="diff-step-explanation active" data-diff-explain="0">
          Starting from pure random noise — no discernible pattern. This is the
          equivalent of fully diffused ink in water.
        </p>
        <p class="diff-step-explanation" data-diff-explain="1">
          The model begins its first denoising pass. It has learned
          statistical patterns from millions of images during training.
        </p>
        <p class="diff-step-explanation" data-diff-explain="2">
          Broad shapes begin to emerge. The text prompt &ldquo;snowman face on blue background&rdquo;
          guides the model via cross-attention with a CLIP text encoder.
        </p>
        <p class="diff-step-explanation" data-diff-explain="3">
          Colors start separating &mdash; whites clustering where the face should be,
          blues filling the background region.
        </p>
        <p class="diff-step-explanation" data-diff-explain="4">
          The pattern becomes clearly recognizable. Each denoising step refines
          the prediction from the previous step.
        </p>
        <p class="diff-step-explanation" data-diff-explain="5">
          Almost there. Fine details resolve. In practice, diffusion models
          run 20-50 of these denoising steps.
        </p>
        <p class="diff-step-explanation" data-diff-explain="6">
          The final denoised image &mdash; a snowman face on a blue background, generated
          entirely from noise, guided only by text.
        </p>
      </div>
    </div>
  </div>
</section>

<style>
  .diff-section {
    position: relative;
    width: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
    padding: 4rem 1.5rem;
  }

  .diff-container {
    max-width: 720px;
    width: 100%;
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 2.5rem;
  }

  .diff-text-content {
    text-align: center;
    max-width: 600px;
  }

  .diff-heading {
    font-size: clamp(1.6rem, 4vw, 2.4rem);
    font-weight: 700;
    color: #1a2e3a;
    margin: 0 0 1rem;
    letter-spacing: -0.02em;
    line-height: 1.2;
  }

  .diff-description {
    font-size: clamp(0.95rem, 2vw, 1.1rem);
    color: #3d5a6a;
    line-height: 1.7;
    margin: 0;
  }

  .diff-description em {
    color: #1a2e3a;
    font-style: italic;
  }

  /* === Visualization wrapper === */
  .diff-viz-wrapper {
    width: 100%;
    display: flex;
    flex-direction: column;
    align-items: center;
    gap: 1.25rem;
    background: rgba(255, 255, 255, 0.7);
    border: 1px solid rgba(8, 145, 178, 0.15);
    border-radius: 12px;
    padding: 1.5rem;
  }

  /* === Prompt display === */
  .diff-prompt {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.6rem 1rem;
    background: rgba(14, 165, 233, 0.06);
    border: 1px solid rgba(14, 165, 233, 0.15);
    border-radius: 8px;
    width: 100%;
    max-width: 440px;
  }

  .diff-prompt-icon {
    color: #0ea5e9;
    font-family: 'Inter', monospace;
    font-weight: 700;
    font-size: 0.9rem;
    flex-shrink: 0;
  }

  .diff-prompt-text {
    color: #1a2e3a;
    font-family: 'Inter', monospace;
    font-size: 0.85rem;
    letter-spacing: 0.01em;
  }

  /* === Step indicator dots === */
  .diff-step-indicator {
    display: flex;
    gap: 0.5rem;
    align-items: center;
  }

  .diff-step-dot {
    display: flex;
    align-items: center;
    justify-content: center;
    width: 28px;
    height: 28px;
    background: none;
    border: none;
    cursor: pointer;
    padding: 0;
  }

  .diff-step-dot-inner {
    display: block;
    width: 8px;
    height: 8px;
    border-radius: 50%;
    background: #c1d5dc;
    transition: background 0.3s ease, transform 0.3s ease, box-shadow 0.3s ease;
  }

  .diff-step-dot.active .diff-step-dot-inner {
    background: #0ea5e9;
    transform: scale(1.4);
    box-shadow: 0 0 8px rgba(14, 165, 233, 0.5);
  }

  .diff-step-dot.completed .diff-step-dot-inner {
    background: #0ea5e9;
  }

  /* === Step label === */
  .diff-step-label {
    font-size: 0.8rem;
    color: #3d5a6a;
    font-weight: 500;
    letter-spacing: 0.04em;
    text-transform: uppercase;
    font-family: 'Inter', system-ui, sans-serif;
    min-height: 1.2em;
  }

  /* === Grid container === */
  .diff-grid-container {
    position: relative;
    width: 100%;
    max-width: 380px;
    aspect-ratio: 1;
  }

  .diff-canvas {
    width: 100%;
    height: 100%;
    border-radius: 6px;
    image-rendering: pixelated;
    image-rendering: crisp-edges;
  }

  /* === Progress bar === */
  .diff-progress-track {
    width: 100%;
    max-width: 380px;
    display: flex;
    flex-direction: column;
    gap: 0.4rem;
  }

  .diff-progress-fill {
    height: 3px;
    background: linear-gradient(90deg, #0ea5e9, #00d4ff);
    border-radius: 2px;
    width: 0%;
    transition: width 0.4s ease;
    box-shadow: 0 0 8px rgba(14, 165, 233, 0.3);
  }

  .diff-progress-labels {
    display: flex;
    justify-content: space-between;
  }

  .diff-progress-label-start,
  .diff-progress-label-end {
    font-size: 0.7rem;
    color: #5a7a8a;
    font-weight: 500;
    text-transform: uppercase;
    letter-spacing: 0.06em;
    font-family: 'Inter', system-ui, sans-serif;
  }

  /* === Step explanations === */
  .diff-step-explanations {
    width: 100%;
    max-width: 440px;
    min-height: 4rem;
    position: relative;
  }

  .diff-step-explanation {
    color: #3d5a6a;
    font-size: 0.9rem;
    line-height: 1.6;
    margin: 0;
    opacity: 0;
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    transition: opacity 0.35s ease;
    pointer-events: none;
  }

  .diff-step-explanation.active {
    opacity: 1;
    position: relative;
    pointer-events: auto;
  }

  /* === Reduced motion === */
  @media (prefers-reduced-motion: reduce) {
    .diff-step-dot-inner,
    .diff-progress-fill,
    .diff-step-explanation {
      transition: none;
    }
  }

  /* === Responsive === */
  @media (max-width: 640px) {
    .diff-section {
      padding: 2rem 1rem;
    }

    .diff-viz-wrapper {
      padding: 1rem;
    }

    .diff-grid-container {
      max-width: 300px;
    }

    .diff-prompt-text {
      font-size: 0.78rem;
    }

    .diff-heading {
      font-size: 1.4rem;
    }
  }
</style>

<script>
  /**
   * DiffusionViz — Canvas-based denoising visualization.
   *
   * Pre-computes 7 step frames at 64x64 resolution using a proper 5-pointed
   * star polygon with point-in-polygon hit testing. Denoising uses per-pixel
   * lerp with center-outward spatial coherence bias.
   *
   * ScrollTrigger.create with onUpdate maps scroll progress to step index.
   */
  function initDiffusionViz() {
    const RES = 64;
    const STEPS = 7;

    // --- Target image colors ---
    const BG_R = 135, BG_G = 195, BG_B = 230;       // light sky blue
    const HEAD_R = 250, HEAD_G = 250, HEAD_B = 252;  // white/snow
    const EYE_R = 35, EYE_G = 35, EYE_B = 40;       // dark charcoal
    const NOSE_R = 230, NOSE_G = 120, NOSE_B = 30;   // carrot orange
    const MOUTH_R = 45, MOUTH_G = 45, MOUTH_B = 50;  // dark

    // --- Snowman face geometry at 64x64 ---
    const cx = RES / 2;
    const cy = RES / 2;
    const headR = RES * 0.38; // head circle radius

    function dist(x1: number, y1: number, x2: number, y2: number): number {
      return Math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2);
    }

    // Eyes: two circles
    const eyeR = 2.5;
    const eyeY = cy - 5;
    const leftEyeX = cx - 7;
    const rightEyeX = cx + 7;

    // Nose: small triangle pointing down-right from center
    function isInsideNose(px: number, py: number): boolean {
      // Triangle: tip pointing down, centered horizontally
      const noseTop = cy - 1;
      const noseBot = cy + 5;
      const noseCx = cx;
      const noseHalfW = 3;
      if (py < noseTop || py > noseBot) return false;
      const t = (py - noseTop) / (noseBot - noseTop);
      const halfW = noseHalfW * (1 - t * 0.7); // narrows toward tip
      return px >= noseCx - halfW && px <= noseCx + halfW;
    }

    // Mouth: arc of dots below the nose
    function isInsideMouth(px: number, py: number): boolean {
      const mouthCy = cy + 10;
      const mouthR = 8;
      const mouthThick = 1.5;
      const d = dist(px, py, cx, mouthCy);
      // Only bottom half of the arc (smile)
      if (py < mouthCy) return false;
      return d >= mouthR - mouthThick && d <= mouthR + mouthThick;
    }

    // --- Pre-compute target image and distance map ---
    const targetImage = new Uint8Array(RES * RES * 3);
    const distMap = new Float32Array(RES * RES);
    const maxDist = Math.sqrt(cx * cx + cy * cy);

    for (let y = 0; y < RES; y++) {
      for (let x = 0; x < RES; x++) {
        const idx = y * RES + x;
        const px = x + 0.5;
        const py = y + 0.5;
        const i3 = idx * 3;

        let r = BG_R, g = BG_G, b = BG_B;

        // Head circle
        if (dist(px, py, cx, cy) <= headR) {
          r = HEAD_R; g = HEAD_G; b = HEAD_B;

          // Eyes
          if (dist(px, py, leftEyeX, eyeY) <= eyeR ||
              dist(px, py, rightEyeX, eyeY) <= eyeR) {
            r = EYE_R; g = EYE_G; b = EYE_B;
          }
          // Nose
          else if (isInsideNose(px, py)) {
            r = NOSE_R; g = NOSE_G; b = NOSE_B;
          }
          // Mouth
          else if (isInsideMouth(px, py)) {
            r = MOUTH_R; g = MOUTH_G; b = MOUTH_B;
          }
        }

        targetImage[i3] = r;
        targetImage[i3 + 1] = g;
        targetImage[i3 + 2] = b;

        // Distance from center, normalized 0..1
        const dx = px - cx;
        const dy = py - cy;
        distMap[idx] = Math.sqrt(dx * dx + dy * dy) / maxDist;
      }
    }

    // --- Seeded PRNG ---
    function mulberry32(seed: number) {
      return function() {
        seed |= 0; seed = seed + 0x6D2B79F5 | 0;
        let t = Math.imul(seed ^ seed >>> 15, 1 | seed);
        t = t + Math.imul(t ^ t >>> 7, 61 | t) ^ t;
        return ((t ^ t >>> 14) >>> 0) / 4294967296;
      };
    }

    const rand = mulberry32(42);

    // --- Generate pure noise (step 0) ---
    const noiseR = new Uint8Array(RES * RES);
    const noiseG = new Uint8Array(RES * RES);
    const noiseB = new Uint8Array(RES * RES);

    for (let i = 0; i < RES * RES; i++) {
      noiseR[i] = Math.floor(rand() * 256);
      noiseG[i] = Math.floor(rand() * 256);
      noiseB[i] = Math.floor(rand() * 256);
    }

    // --- Per-pixel random offset for denoising variation ---
    const pixelRandOffset = new Float32Array(RES * RES);
    for (let i = 0; i < RES * RES; i++) {
      pixelRandOffset[i] = rand() * 0.3; // up to 0.3 random variation
    }

    // --- Pre-compute step frames as ImageData ---
    const offscreen = document.createElement('canvas');
    offscreen.width = RES;
    offscreen.height = RES;
    const offCtx = offscreen.getContext('2d')!;
    const stepFrames: ImageData[] = [];

    for (let step = 0; step < STEPS; step++) {
      const imageData = offCtx.createImageData(RES, RES);
      const data = imageData.data;
      const t = step / (STEPS - 1); // 0 = noise, 1 = final

      for (let y = 0; y < RES; y++) {
        for (let x = 0; x < RES; x++) {
          const idx = y * RES + x;
          const px = idx * 4;
          const i3 = idx * 3;

          if (step === 0) {
            // Pure noise
            data[px] = noiseR[idx];
            data[px + 1] = noiseG[idx];
            data[px + 2] = noiseB[idx];
            data[px + 3] = 255;
          } else if (step === STEPS - 1) {
            // Final clean image
            data[px] = targetImage[i3];
            data[px + 1] = targetImage[i3 + 1];
            data[px + 2] = targetImage[i3 + 2];
            data[px + 3] = 255;
          } else {
            // Intermediate: center-outward denoising with per-pixel variation
            // Pixels closer to center denoise faster
            // denoiseFactor: 0 = fully noise, 1 = fully target
            const dist = distMap[idx];
            const randOff = pixelRandOffset[idx];

            // Spatial coherence: center pixels resolve first
            // Use a sigmoid-like curve centered around the current step progress
            // adjusted by distance from center
            const adjustedT = t * 1.4 - dist * 0.6 - randOff;
            const denoiseFactor = Math.max(0, Math.min(1, adjustedT * 1.8));

            // Lerp between noise and target
            const nr = noiseR[idx];
            const ng = noiseG[idx];
            const nb = noiseB[idx];
            const tr = targetImage[i3];
            const tg = targetImage[i3 + 1];
            const tb = targetImage[i3 + 2];

            data[px] = Math.round(nr + (tr - nr) * denoiseFactor);
            data[px + 1] = Math.round(ng + (tg - ng) * denoiseFactor);
            data[px + 2] = Math.round(nb + (tb - nb) * denoiseFactor);
            data[px + 3] = 255;
          }
        }
      }

      // Apply a blur pass for intermediate steps to simulate denoising softness
      if (step > 0 && step < STEPS - 1) {
        // Simple box blur: average with neighbors
        const blurStrength = Math.max(0, 1 - t); // stronger blur at earlier steps
        if (blurStrength > 0.1) {
          const src = new Uint8ClampedArray(data);
          for (let y = 1; y < RES - 1; y++) {
            for (let x = 1; x < RES - 1; x++) {
              const px = (y * RES + x) * 4;
              for (let c = 0; c < 3; c++) {
                const center = src[px + c];
                const top = src[((y - 1) * RES + x) * 4 + c];
                const bot = src[((y + 1) * RES + x) * 4 + c];
                const left = src[(y * RES + (x - 1)) * 4 + c];
                const right = src[(y * RES + (x + 1)) * 4 + c];
                const avg = (top + bot + left + right) / 4;
                data[px + c] = Math.round(center + (avg - center) * blurStrength * 0.5);
              }
            }
          }
        }
      }

      stepFrames.push(imageData);
    }

    // --- Set up visible canvas ---
    const canvas = document.getElementById('diffusion-canvas') as HTMLCanvasElement | null;
    if (!canvas) return;

    canvas.width = RES;
    canvas.height = RES;
    const ctx = canvas.getContext('2d')!;

    // Draw initial frame
    ctx.putImageData(stepFrames[0], 0, 0);

    // --- UI state management ---
    const section = document.getElementById('diffusion-section');
    if (!section) return;

    let currentStep = 0;

    const dots = section.querySelectorAll('.diff-step-dot');
    const stepLabel = section.querySelector('[data-diff="step-label"]');
    const progressFill = section.querySelector('[data-diff="progress-fill"]') as HTMLElement;
    const explanations = section.querySelectorAll('.diff-step-explanation');

    const stepLabelsText = [
      'Step 0: Pure noise',
      'Step 1: Faint structure',
      'Step 2: Shapes emerge',
      'Step 3: Colors coalesce',
      'Step 4: Pattern forming',
      'Step 5: Nearly resolved',
      'Step 6: Final image',
    ];

    function setStep(step: number) {
      const s = Math.max(0, Math.min(step, STEPS - 1));
      if (s === currentStep) return;
      currentStep = s;

      // Draw the pre-computed frame
      ctx.putImageData(stepFrames[s], 0, 0);

      // Update dots
      dots.forEach((d, i) => {
        d.classList.toggle('active', i === s);
        d.classList.toggle('completed', i < s);
      });

      // Update label
      if (stepLabel) stepLabel.textContent = stepLabelsText[s];

      // Update progress bar
      if (progressFill) progressFill.style.width = `${(s / (STEPS - 1)) * 100}%`;

      // Update explanations
      explanations.forEach((e, i) => e.classList.toggle('active', i === s));
    }

    // Click dots to jump
    dots.forEach((dot, i) => dot.addEventListener('click', () => setStep(i)));

    // --- ScrollTrigger integration ---
    const gsap = (window as any).gsap;
    const ScrollTrigger = (window as any).ScrollTrigger;

    if (!gsap || !ScrollTrigger) {
      // Fallback: show final image and all explanations visible
      ctx.putImageData(stepFrames[STEPS - 1], 0, 0);
      explanations.forEach(el => {
        (el as HTMLElement).style.opacity = '1';
        (el as HTMLElement).style.position = 'relative';
      });
      return;
    }

    // Auto-play animation when section enters viewport
    let autoPlayInterval: ReturnType<typeof setInterval> | null = null;
    let hasAutoPlayed = false;

    ScrollTrigger.create({
      trigger: section,
      start: 'top 70%',
      end: 'bottom 30%',
      onEnter: () => {
        if (hasAutoPlayed) return;
        hasAutoPlayed = true;
        let step = 0;
        setStep(0);
        autoPlayInterval = setInterval(() => {
          step++;
          if (step >= STEPS) {
            if (autoPlayInterval) clearInterval(autoPlayInterval);
            autoPlayInterval = null;
            return;
          }
          setStep(step);
        }, 2000);
      },
      onLeaveBack: () => {
        // Reset if user scrolls back above
        if (autoPlayInterval) {
          clearInterval(autoPlayInterval);
          autoPlayInterval = null;
        }
        hasAutoPlayed = false;
        setStep(0);
      },
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', () => requestAnimationFrame(initDiffusionViz));
  } else {
    requestAnimationFrame(initDiffusionViz);
  }
</script>
